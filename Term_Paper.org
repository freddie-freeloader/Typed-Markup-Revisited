
* Final encoding for extensible markup languages
  
** Abstract

In the age of digital documents, an author of content is confronted with the
question which document format to choose [when it comes to text].

Since every document format has its advantages, one might not want to commit to
a specific format to soon. A series of blog posts might turn into a book (or at
least a pretty typeset ~pdf~) or an author might want to give the reader the
freedom to read their text on differently sized displays — if the reader has
ever tried to read a paper in ~pdf~-format on an e-book reader, no further
motivation might be needed.

Luckily the problem of decoupling initial text from output seems to be solved by
the rise of markup languages such as Markdown/Commonmark and alike. These type
of documents can be easily [transpiled/converted] into all sorts of output
formats by programs as ~pandoc~. 

If the reader has no objections to such a publishing system, they might read no
further and write away their next /format-agnostic/ document. But if they are
interested in [bridging the gap between markup and programming languages and/or]
how they can let a type-checker reason about the /well-formedness/ of their
document, they may find the findings gathered in this paper worth while.

This paper mostly outlines the ideas of the work on ~HSXML: Typed SXML~ and the
underlying idea of ~Finally Tagless Interpreters~.

In short a richly typed representation like ~HSXML~ has in our opinion two major
advantages over markup languages such as Markdown/Commonmark et al.:

1. Guarantee the well-formedness of the document by construction
2. Easy extensibility without loosing the guarantees of 1.

While having these two advantages we still do not want to loose perspective and
be true to our initial goal:

1. Writing documents that are format agnostic — i.e. observe our source in
   different ways

or as described in the Wikipedia-article on /Markup Languages/

#+BEGIN_QUOTE
Descriptive markup

Markup is used to label parts of the document rather than to provide specific
instructions as to how they should be processed. Well-known examples include
LaTeX, HTML, and XML. The objective is to decouple the inherent structure of the
document from any particular treatment or rendition of it. Such markup is often
described as "semantic".
#+END_QUOTE

Oleg Kiselyov might want to argue that such a markup is even /symantic/ (TODO:
add reference).

*** TODO
    
Maybe we should mention the the idea of XML is/was exactly that:

(From [[https://docs.microsoft.com/en-us/sql/relational-databases/xml/compare-typed-xml-to-untyped-xml][this website]]):

#+BEGIN_QUOTE
An XML schema provides the following:

- *Validation constraints*: Whenever a typed xml instance is assigned to or
  modified, SQL Server validates the instance.

- *Data type information*: Schemas provide information about the types of
  attributes and elements in the xml data type instance. The type information
  provides more precise operational semantics to the values contained in the
  instance than is possible with untyped xml. For example, decimal arithmetic
  operations can be performed on a decimal value, but not on a string value.
  Because of this, typed XML storage can be made significantly more compact than
  untyped XML.  
#+END_QUOTE

** Introduction
**** TODO Explanation
(Explain variants/constructors versus writers/observations)
In this chapter we will give a quick overview on the challenges related to the
representation of markup languages. 

We will not concern ourselves on how the end-user will construct such
expressions, but to provide a short comment on that matter: If we construct our
intermediate representation, this could either be constructed directly in the
host language as an (shallow/deep?) embedded DSL or we could write some parsers
that create our representation in an automated way and hide therefore the
implementation details from the user. In the tutorial paper Kiselyov et al. show
that the flexible extensibility of the representation leads to the flexible
extensibility of the corresponding parsers.


*** AST-encoding with Extensible Observers

Pandoc achieves the separation of input and output format by choosing an
Algebraic Data Type as its intermediate representation. We will quickly sketch
why such an encoding leads to an easy extensibility of constructors by looking a
subset of Pandoc's Abstract Syntax Tree and writing some observers for it.
    
Given the representation [ref. AST Definition] we can write /writers/ that
interpret this data in different ways [ref. Markdown & LaTeX/HTML Observer]. So
in the dimension of observers our encoding is obviously extensible.

Now we can construct a tree in the host language and interpret it in two
different ways:

[Example without ~IsString~ instance]
#+NAME: AST Definition
#+BEGIN_SRC haskell :session pandocRepl
groceryList :: [Doc]
groceryList
  = [ Heading 1  [Str "Grocery list"]
    , BulletList [ Paragraph [Str "1 Banana"]
                 , Paragraph [Str "2 ", Emph [Str "fresh"], Str " Apples"]]]
#+END_SRC

We can make our life a bit easier by adding an instance for ~IsString~ for our
output data types. This injects ~String~ automatically into our data-types by
applying ~fromString~ to it.

#+NAME: AST Definition
#+BEGIN_SRC haskell :session pandocRepl
instance IsString Inline where
  fromString = Str
#+END_SRC


The code from above is now even more concise.

#+NAME: AST Definition
#+BEGIN_SRC haskell :session pandocRepl
groceryListShort :: [Doc]
groceryListShort
  = [ Heading 1  ["Grocery list"]
    , BulletList [ Paragraph ["1 Banana"]
                 , Paragraph ["2 ", Emph ["fresh"], " Apples"]]]
#+END_SRC

*** Extensible Variants

The encoding works very well, as long as we have foreseen every variant we might
want to create. But as soon as we want to add a new kind of node (e.g. a node
representing the em dash) we are out of luck. Even if we have access to the
original ADT-definition and we could add this new node, this would break all
existing observers that were written for the original ADT.
    
*** Expression Problem

To be extensible in the dimension of observers as well as the dimension of the
variants, while still guaranteeing statically their compatibility, is quite a
challenge and one that quite common when writing software. It was named as the
*Expression Problem* by Wadler [reference] and many solutions have been proposed.

The most prominent solutions — that are right at home in Haskell — are described
in /Data-types a la carte/ [reference] and in /Finally Tagless …/ [reference].
Kiselyov’s et al. solution to this is — in our opinion — both easy to use and
when used as a DSL for our particular problem, the relationship to S-expressions
becomes quite obvious.
    
**** TODO Tag code and reference
     
Code is from
[[https://github.com/jgm/pandoc-types/blob/master/Text/Pandoc/Definition.hs]]
#+NAME: AST Definition
#+BEGIN_SRC haskell :session pandocEncoding
data Doc
  = Paragraph [Inline]    -- ^ Paragraph
  | BulletList [Doc]      -- ^ Bullet list (list of items, each a list
                          --   of blocks)
  | Heading Int [Inline]  -- ^ Heading - level (integer) and text (inlines)

data Inline
  = Str String    -- ^ Text (string)
  | Emph [Inline] -- ^ Emphasized text (list of inlines)
#+END_SRC
    
#+NAME: Pandoc-encoding — Markdown Observer
#+BEGIN_SRC haskell :session pandocEncoding
type Markdown = String

docToMd :: Doc -> Markdown
docToMd (Paragraph text) = concatMap inlineToMd text
docToMd (BulletList docs) = concatMap (("- " ++) . docToMd) docs ++ "\n"
docToMd (Heading level text) = headingPrefix ++ concatMap inlineToMd text
  where
    headingPrefix = concat $ replicate level "#"

inlineToMd :: Inline -> Markdown
inlineToMd (Str content) = content
inlineToMd (Emph contents) = "*" ++ concatMap inlineToMd contents ++ "*"
#+END_SRC

#+NAME: Pandoc-encoding — LaTeX Observer
#+BEGIN_SRC haskell :session pandocEncoding
type LaTeX = String

docToLaTex :: Doc -> LaTeX
...

inlineToLaTex :: Inline -> LaTeX
...
#+END_SRC

*** Final Tagless Encoding

Our first attempt to encode our document in the final tagless encoding will not
have the distinction between ~Doc~ and ~Inline~ — which was enforced by the
Pandoc-encoding. But later we will see that we are able to recover that property
quite easily with great extensibility properties.

The basic idea of the final tagless encoding is as follows:

- Create a type class that specifies all our constructors in Church encoding
  [ref. first code snippet]
- Parametrize over the return-type and recursive fields of those constructors
  [ref. the first encoding (from ~FirstFTEncoding.hs~)]

#+NAME: First FT-encoding — Classes
#+BEGIN_SRC haskell :session firstStepFT
class Doc where
  paragraph  :: [String] -> String
  bulletList :: [String] -> String
  heading    :: Int -> [String] -> String

class Inline String where
  str :: String -> String
  emph :: [String] -> String
#+END_SRC

#+NAME: First FT-encoding — Classes
#+BEGIN_SRC haskell :session firstFTEncoding
class Doc a where
  paragraph  :: [a] -> a
  bulletList :: [a] -> a
  heading    :: Int -> [a] -> a

class Inline a where
  str :: String -> a
  emph :: [a] -> a
#+END_SRC

Out type classes look basically like a GADT-encoding where all recursive
occurences and the return-type are parametrized over.

The observers will now be instances of theses type classes. The reader might
notice that we cannot use the same carrier type for different interpretations of
our AST — otherwise we would get overlapping instances. This can be quite easily
solved by wrapping the carrier type into a ~newtype~ and add or derive the
needed instances for it. In our case ~Markdown~ is simply a ~newtype~ of
~String~. Therefore the instances for ~IsString~ and ~Monoid~ are
straightforward to implement. [ Add information, on why we need ~Monoid~ ]

#+NAME: First FT-encoding — Markdown
#+BEGIN_SRC haskell :session firstFTEncoding
instance Doc Markdown where
  paragraph     = mconcat
  bulletList    = addLineBreak . mconcat . map (mappend "\n- ")
  heading level = addLineBreak . mappend (mconcat $ replicate level "#" ) . mconcat

instance Inline Markdown where
  str = fromString
  emph texts = "*" `mappend` mconcat texts `mappend` "*"
#+END_SRC
    
Let's see how our example from above looks in our new encoding:
    
#+NAME: First FT-encoding — Markdown
#+BEGIN_SRC haskell :session firstFTEncoding
groceryList
  = [ heading 1  [str "Grocery list"]
    , bulletList [ paragraph [str "1 Banana"]]
                 , paragraph [str "2 ", emph [str "fresh"], str " Apples"] ]
#+END_SRC
    
[ Write something about ~NoMonomorphismRestriction~ ]

As before, we can automate the injection of ~String~ into our encoding by using
the ~OverloadedStrings~ language pragma. We do this be adding a constraint on
the type classes, so every output format must have an ~IsString~ instance.

Interestingly ~Doc~ has now no dependency on ~Inline~ anymore. In a way this is
not ideal, since we can now construct the following:

#+NAME: First FT-encoding — Markdown
#+BEGIN_SRC haskell :session firstFTEncoding
badHeading = [ heading 1  [ heading 2 [str "Headingception!!"] ] ]
#+END_SRC

As noted above, we lost the distinction between ~Doc~ and ~Inline~. But we also
gained something — ~Doc~ can now be used without ~Inline~ and we can now also
add new nodes without changing our original data types:

#+NAME: First FT-encoding — Markdown
#+BEGIN_SRC haskell :session firstFTEncoding
class IsString a => MoreStyles a where
  strong :: [a] -> a
  strikethrough :: [a] -> a
#+END_SRC

Not only can we now mix those node types at will, but the type of an expression
will reflect which type classes (i.e. algebras) we used for constructing it: 

#+NAME: First FT-encoding — Markdown
#+BEGIN_SRC haskell :session firstFTEncoding
stylishText :: (Inline a, MoreStyles a) => a
stylishText = strong [str "Green Tea keeps me awake"]
#+END_SRC

That is why the type system can now statically tell us whether we can evaluate
~stylishText~ to a particular type. If we wanted to evaluate an expression, that
uses constructors that belong to a type class ~X~ and we would want to evaluate
the expression to some carrier type ~C~, ~C~ has to be instance of ~X~. Since
this is a static property, it can be decided at compile time.

*** Recover Context Awareness

To regain the context awareness of the Pandoc encoding, we add another field
~ctx~ to our ~Doc~ wrapper.












    














** Overview
   
   In this paper we will start with introducing ~HSMLX~ — a variant of typed ~XML~
that uses S-expressions as its syntax — and we will write first constructors and
observers for it in Haskell.

*** SXML

Basically ~SXML~ is just 
    
** Writing interpreters for typed mark-up

In this chapter we will write two representations for ~HSXML~, compare their
advantages and disadvantages, and in the end try to come up with an encoding
that uses the strengths of both to overcome their shortcomings.

*** HSXML

Let us first have a look, with what kind of data we are dealing with.

#+BEGIN_SRC xml
<block>
  <h1>Todo List</h1>
  <items>
    <item>Finish term paper</item>
    <item>Reimplemented HSXML</item>
  </items>
</block>
#+END_SRC

#+BEGIN_SRC haskell
(block [ h1 "Todo List"
       , items [

#+END_SRC

(Some introduction to ~HSXMLS~)

*** Initial Representation
*** Final Representation
*** Ad-hoc polymorphism to the rescue *or* Final encoding with class

** The decoding problem / No homo!
