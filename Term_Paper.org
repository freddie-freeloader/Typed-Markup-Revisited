#+TITLE: HSXML/Typed Markup Revisited
#+SUBTITLE: Final encoding for extensible markup languages
#+AUTHOR: Jonas U. Benn
#+EMAIL: mail@benn.in
#+LATEX_CLASS: article
#+OPTIONS: tasks:nil

* Final encoding for extensible markup languages

** Abstract

In the age of digital documents, an author of content is confronted with the
question which document format to choose [when it comes to text].

Since every document format has its advantages, one might not want to commit to
a specific format to soon. A series of blog posts might turn into a book (or at
least a pretty typeset ~pdf~) or an author might want to give the reader the
freedom to read their text on differently sized displays — if the reader has
ever tried to read a paper in ~pdf~-format on an e-book reader, no further
motivation might be needed.

Luckily the problem of decoupling initial text from output seems to be solved by
the rise of markup languages such as Markdown/Commonmark and alike. These type
of documents can be easily [transpiled/converted] into all sorts of output
formats by programs as ~pandoc~.

If the reader has no objections to such a publishing system, they might read no
further and write away their next /format-agnostic/ document. But if they are
interested in [bridging the gap between markup and programming languages and/or]
how they can let a type-checker reason about the /well-formedness/ of their
document, they may find the findings gathered in this paper worth while.

This paper mostly outlines the ideas of the work on ~HSXML: Typed SXML~ and the
underlying idea of ~Finally Tagless Interpreters~.

In short a richly typed representation like ~HSXML~ has in our opinion two major
advantages over markup languages such as Markdown/Commonmark et al.:

1. Guarantee the well-formedness of the document by construction
2. Easy extensibility without loosing the guarantees of 1.

While having these two advantages we still do not want to loose perspective and
be true to our initial goal:

1. Writing documents that are format agnostic — i.e. observe our source in
   different ways

or as described in the Wikipedia-article on /Markup Languages/

#+BEGIN_QUOTE
Descriptive markup

Markup is used to label parts of the document rather than to provide specific
instructions as to how they should be processed. Well-known examples include
LaTeX, HTML, and XML. The objective is to decouple the inherent structure of the
document from any particular treatment or rendition of it. Such markup is often
described as "semantic".
#+END_QUOTE

Oleg Kiselyov might want to argue that such a markup is even /symantic/.
**** TODO Add reference



**** TODO Explain relationship to XML with schema

Maybe we should mention the the idea of XML is/was exactly that:

(From [[https://docs.microsoft.com/en-us/sql/relational-databases/xml/compare-typed-xml-to-untyped-xml][this website]]):

#+BEGIN_QUOTE
An XML schema provides the following:

- *Validation constraints*: Whenever a typed xml instance is assigned to or
  modified, SQL Server validates the instance.

- *Data type information*: Schemas provide information about the types of
  attributes and elements in the xml data type instance. The type information
  provides more precise operational semantics to the values contained in the
  instance than is possible with untyped xml. For example, decimal arithmetic
  operations can be performed on a decimal value, but not on a string value.
  Because of this, typed XML storage can be made significantly more compact than
  untyped XML.
#+END_QUOTE

** Introduction
**** TODO Explanation
(Explain variants/constructors versus writers/observations)
In this chapter we will give a quick overview on the challenges related to the
representation of markup languages.

We will not concern ourselves on how the end-user will construct such
expressions, but to provide a short comment on that matter: If we construct our
intermediate representation, this could either be constructed directly in the
host language as an (shallow/deep?) embedded DSL or we could write some parsers
that create our representation in an automated way and hide therefore the
implementation details from the user. In the tutorial paper Kiselyov et al. show
that the flexible extensibility of the representation leads to the flexible
extensibility of the corresponding parsers.


*** AST-encoding with Extensible Observers

Pandoc achieves the separation of input and output format by choosing an
Algebraic Data Type as its intermediate representation. We will quickly sketch
why such an encoding leads to an easy extensibility of constructors by looking a
subset of Pandoc's Abstract Syntax Tree and writing some observers for it.

Given the representation [[code:Pandoc_AST_Definition]] we can write /observers/ that
interpret this data in different ways [[code:Pandoc_Observers]]. So
in the dimension of observers our encoding is obviously extensible.

Now we can construct a tree in the host language and interpret it in two
different ways:

#+CAPTION: First Example Verbose
#+NAME: code:Pandoc_Example_Verbose
#+BEGIN_SRC haskell :session pandocRepl
groceryList :: [Block]
groceryList
  = [ Heading 1  [ Str "Grocery list"]
    , BulletList [ Paragraph [Str "1 Banana"]
                 , Paragraph [Str "2 ", Emph [Str "organic"], Str " Apples"]]]

groceryListCM :: CommonMark
groceryListCM = mconcatMap docToCMark groceryList

groceryListLaTeX :: LaTeX
groceryListLaTeX = mconcatMap docToLaTeX groceryList
#+END_SRC

We can make our life a bit easier by adding an instance for ~IsString~ for our
representation. This injects ~String~ automatically into our data types by
applying ~fromString~ to it.

#+BEGIN_SRC haskell :session pandocRepl
instance IsString Inline where
  fromString = Str
#+END_SRC


Our initial definition is now even more concise:

#+NAME: code:Pandoc_Example_Short
#+BEGIN_SRC haskell :session pandocRepl
groceryListShort :: [Block]
groceryListShort
  = [ Heading 1  [ "Grocery list"]
    , BulletList [ Paragraph ["1 Banana"]
                 , Paragraph ["2 ", Emph ["organic"], " Apples"] ]]
#+END_SRC

*** Extensible Variants

The encoding works very well, as long as we have foreseen every variant we might
want to create. But as soon as we want to add a new kind of node (e.g. a node
representing the em dash) we are out of luck. Even if we have access to the
original ADT-definition and we could add this new node, this would break all
existing observers that were written for the original ADT.

*** Expression Problem

To be extensible in the dimension of observers as well as the dimension of the
variants, while still guaranteeing statically their compatibility, is quite a
challenge and one that quite common when writing software. It was named as the
*Expression Problem* by Wadler [reference] and many solutions have been proposed.

The most prominent solutions — that are right at home in Haskell — are described
in /Data-types a la carte/ [reference] and in /Finally Tagless …/ [reference].
Kiselyov’s et al. solution to this is — in our opinion — both easy to use and
when used as a DSL for our particular problem, the relationship to S-expressions
becomes quite obvious.

#+CAPTION: This is part of the pandoc AST modulo EmDash
#+NAME: code:Pandoc_AST_Definition
#+BEGIN_SRC haskell :session pandocEncoding
data Block
  = Paragraph   [Inline] -- ^ Paragraph
  | BulletList  [Block]    -- ^ Bullet list (list of items, each a block)
  | Heading Int [Inline] -- ^ Heading - level (integer) and text (inlines)

data Inline
  = Str String      -- ^ Text (string)
  | EmDash          -- ^ em dash
  | Emph   [Inline] -- ^ Emphasized text (list of inlines)
  | Strong [Inline] -- ^ Strongly emphasized text (list of inlines)
#+END_SRC

#+CAPTION: Pandoc-encoding — Markdown Observer
#+NAME: code:Pandoc_Observers
#+BEGIN_SRC haskell :session pandocEncoding
docToCMark :: Block -> CommonMark
docToCMark (Paragraph text)     = mconcatMap inlineToCMark text
docToCMark (BulletList docs)    = addLineBreak $ mconcatMap (mappend "- " . docToCMark) docs
docToCMark (Heading level text) = addLineBreak $ headingPrefix `mappend` mconcatMap inlineToCMark text
 where
  headingPrefix = mconcat $ replicate level "#"

addLineBreak :: CommonMark -> CommonMark
addLineBreak text = text `mappend` "\n"

inlineToCMark :: Inline -> CommonMark
inlineToCMark (Str content)     = fromString content
inlineToCMARK (Emph contents)   = "*" `mappend` mconcatMap inlineToCMark contents `mappend` "*"
inlineToCMARK (Strong contents) = "**" `mappend` mconcatMap inlineToCMark contents `mappend` "**"
inlineToCMARK EmDash            = "---"
#+END_SRC

#+NAME: Pandoc-encoding — LaTeX Observer
#+BEGIN_SRC haskell :session pandocEncoding
docToLaTex :: Block -> LaTeX
...

inlineToLaTex :: Inline -> LaTeX
...
#+END_SRC

*** Final Tagless Encoding

Our first attempt to encode our document in the final tagless encoding will not
have the distinction between ~Doc~ and ~Inline~ — which was enforced by the
Pandoc-encoding. But later we will see that we are able to recover that property
quite easily with great extensibility properties.

The basic idea of the final tagless encoding is as follows:

- Create a type class that specifies all our constructors in Church encoding
  [[code:First_Step_FT-encoding]]
- Parametrize over the return-type and recursive fields of those constructors
  [[code:Second_Step_FT-encoding]]

#+CAPTION: First Step FT-encoding
#+NAME: code:First_Step_FT-encoding
#+BEGIN_SRC haskell :session firstStepFT
data Doc where
  Doc :: String -> Doc

instance Monoid (Doc doc) where
  mappend (Doc doc1) (Doc doc2) = Doc $ doc1 `mappend` doc2
  mempty = Doc mempty

-- Constructors

class Block where
  paragraph  ::        [Doc] -> Doc
  bulletList ::        [Doc] -> Doc
  heading    :: Int -> [Doc] -> Doc

class Inline a where
  emDash ::           Doc
  str    :: String -> Doc
  str = Doc
#+END_SRC

#+CAPTION: Second Step FT-encoding
#+NAME: code:Second_Step_FT-encoding
#+BEGIN_SRC haskell :session firstFTEncoding
-- DocConstraint defined using ConstraintKinds
type DocConstraint doc = (Monoid doc, IsString doc)

newtype Doc doc = Doc doc

instance DocConstraint doc => -- Have to restrict for the use of 'mempty'
  Monoid (Doc doc) where
  mappend (Doc doc1) (Doc doc2) = Doc $ doc1 `mappend` doc2
  mempty = Doc mempty

-- Constructors

class Block a where
  paragraph  ::        [Doc a] -> Doc a
  bulletList ::        [Doc a] -> Doc a
  heading    :: Int -> [Doc a] -> Doc a

class DocConstraint a =>
  Inline a where
  emDash ::           Doc a
  str    :: String -> Doc a
  str = Doc . fromString
#+END_SRC

The type classes look basically like a GADT-encoding where all recursive
occurrences and the return-type are parametrized over.

The observers will now be instances of theses type classes. The reader might
notice that we cannot use the same carrier type for different interpretations of
our AST — otherwise we would get overlapping instances. This can be quite easily
solved by wrapping the carrier type into a ~newtype~ and add or derive the
needed instances for it. In our case ~Markdown~ is simply a ~newtype~ of
~String~. Therefore the instances for ~IsString~ and ~Monoid~ are
straightforward to implement. [ Add information, on why we need ~Monoid~ ]

#+NAME: First_FT_Instance
#+BEGIN_SRC haskell :session firstFTEncoding
instance Block CommonMark where
  paragraph     = mconcat
  bulletList    = addLineBreak . mconcat . map (mappend "\n- ")
  heading level = addLineBreak . mappend (mconcat $ replicate level "#") . mconcat

addLineBreak :: DocConstraint doc => doc -> doc
addLineBreak text = text `mappend` "\n"
#+END_SRC

Let's see how our example from above looks in our new encoding:

#+NAME: FT_example_verbose
#+BEGIN_SRC haskell :session firstFTEncoding
groceryList
  = [ heading 1  [str "Grocery list"]
    , bulletList [ paragraph [str "1 Banana"]]
                 , paragraph [str "2 ", emph [str "fresh"], str " Apples"] ]
#+END_SRC

[ Write something about ~NoMonomorphismRestriction~ ]

As before, we can automate the injection of ~String~ into our encoding by using
the ~OverloadedStrings~ language pragma. We do this be adding a constraint on
the type classes, so every output format must have an ~IsString~ instance.

Interestingly ~Doc~ has now no dependency on ~Inline~ anymore. In a way this is
not ideal, since we can now construct the following:

#+NAME: Malformed_heading
#+BEGIN_SRC haskell :session firstFTEncoding
badHeading = [ heading 1  [ heading 2 [str "Headingception!!"] ] ]
#+END_SRC

As noted above, we lost the distinction between ~Doc~ and ~Inline~. But we also
gained something — ~Doc~ can now be used without ~Inline~ and we can now also
add new nodes without changing our original data types:

#+NAME: MoreStyles
#+BEGIN_SRC haskell :session firstFTEncoding
class IsString a => MoreStyles a where
  strong :: [a] -> a
  strikethrough :: [a] -> a
#+END_SRC

Not only can we now mix those node types at will, but the type of an expression
will reflect which type classes (i.e. algebras) we used for constructing it:

#+NAME: First FT-encoding — Markdown
#+BEGIN_SRC haskell :session firstFTEncoding
stylishText :: (Inline a, MoreStyles a) => a
stylishText = strong [str "Green Tea keeps me awake"]
#+END_SRC

That is why the type system can now statically tell us whether we can evaluate
~stylishText~ to a particular type. If we wanted to evaluate an expression, that
uses constructors that belong to a type class ~X~ and we would want to evaluate
the expression to some carrier type ~C~, ~C~ has to be instance of ~X~. Since
this is a static property, it can be decided at compile time.

*** Recover Context Awareness

To regain the context awareness of the Pandoc encoding, we add another field
~ctx~ to our ~Doc~ wrapper [[code:Context-aware_wrapper]]. The ~ctx~ is a
phantom/proxy (?) type and with its help, we can specify in which context a
constructor can be used.

As shown before, the first /Final Tagless encoding/ had the disadvantage, that
we could construct a heading inside another heading. To prohibit this, the
~heading~ constructor has the following context-aware definition:

#+CAPTION: Context-aware heading
#+NAME: code:Context-aware_heading
#+BEGIN_SRC haskell :session firstFTEncoding
heading :: Int -> [DocWithCtx InlineCtx doc] -> DocWithCtx BlockCtx doc
#+END_SRC

The type signature states, that the function expects a ~DocWithCtx~-wrapper in
the ~InlineCtx~-context and returns a wrapper in the ~BlockCtx~-context. With
this refined signature a heading inside a heading will be rejected by the type
system.

The set of available contexts should be defined generously, since all
independent extensions of the AST should agree on them. This is obviously are
restriction — but one that might be very valuable.

It is still possible to create context independent constructors. This can be
achieved by parametrizing over the context:

#+CAPTION: Context-independent wrapper
#+NAME: code:Context-independent_constructor
#+BEGIN_SRC haskell :session firstFTEncoding
qed :: DocWithCtx ctx doc
#+END_SRC

#+CAPTION: Context-aware wrapper
#+NAME: code:Context-aware_wrapper
#+BEGIN_SRC haskell :session firstFTEncoding
newtype DocWithCtx ctx doc = DocWithCtx doc
#+END_SRC
